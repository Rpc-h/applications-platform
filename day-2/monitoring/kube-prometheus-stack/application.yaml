apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kube-prometheus-stack
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://prometheus-community.github.io/helm-charts
    targetRevision: "45.7.1"
    chart: kube-prometheus-stack
    plugin:
      env:
        - name: HELM_VALUES
          value: |
            coreDns:
              enabled: false
            kubeDns:
              enabled: true
            kubeProxy:
              enabled: false
            kubeScheduler:
              enabled: false
            kubeControllerManager:
              enabled: false
            alertmanager:
              enabled: true
              ingress:
                enabled: true
                ingressClassName: traefik
                annotations:
                  traefik.ingress.kubernetes.io/router.entrypoints: websecure
                  traefik.ingress.kubernetes.io/router.middlewares: traefik-forward-auth@kubernetescrd
                  cert-manager.io/cluster-issuer: production
                hosts:
                  - alertmanager.ARGOCD_ENV_DOMAIN
                path: /
                tls:
                  - secretName: alertmanager-cert
                    hosts:
                      - alertmanager.ARGOCD_ENV_DOMAIN
              config:
                route:
                  receiver: "null"
                  group_wait: 30s
                  group_interval: 5m
                  repeat_interval: 1h
                  group_by: [alertname, severity, id]
                  routes:
                    - receiver: "null"
                      match:
                        severity: null
                      continue: false
                    - receiver: "telegram"
                      match_re:
                        severity: ^(info|warning|critical)$
                      continue: true
                    - receiver: "info"
                      match:
                        severity: info
                    - receiver: "warning"
                      match:
                        severity: warning
                    - receiver: "critical"
                      match:
                        severity: critical
                receivers:
                  - name: "null"
                  - name: "info"
                    webhook_configs:
                      - url: "http://notification-service.monitoring:8080/alertmanager/room/%21DaMkbMrexCDEDGoiup:hoprnet.io"
                  - name: "warning"
                    webhook_configs:
                      - url: "http://notification-service.monitoring:8080/alertmanager/room/%21WxXYnjzPyglBADfuyc:hoprnet.io"
                  - name: "critical"
                    webhook_configs:
                      - url: "http://notification-service.monitoring:8080/alertmanager/room/%21HgRjXaMRVLzZYZPEFg:hoprnet.io"
                  - name: "telegram"
                    telegram_configs:
                      - chat_id: -1001562396359
                        bot_token: 6201490054:AAEjF_1ahOhmK1ysT2pD9gNRjIjzIwD1JF4
            kubeStateMetrics:
              enabled: true
            nodeExporter:
              enabled: true
            prometheus:
              enabled: true
              ingress:
                enabled: true
                ingressClassName: traefik
                annotations:
                  traefik.ingress.kubernetes.io/router.entrypoints: websecure
                  traefik.ingress.kubernetes.io/router.middlewares: traefik-forward-auth@kubernetescrd
                  cert-manager.io/cluster-issuer: production
                hosts:
                  - prometheus.ARGOCD_ENV_DOMAIN
                path: /
                tls:
                  - secretName: prometheus-cert
                    hosts:
                      - prometheus.ARGOCD_ENV_DOMAIN
              prometheusSpec:
                serviceMonitorSelectorNilUsesHelmValues: false
                storageSpec:
                  volumeClaimTemplate:
                    spec:
                      storageClassName: main
                      resources:
                        requests:
                          storage: 10Gi
            grafana:
              enabled: true
              adminPassword: ARGOCD_ENV_RANDOM_STRING
              persistence:
                enabled: true
                storageClassName: main
                size: 1Gi
              grafana.ini:
                server:
                  root_url: https://grafana.ARGOCD_ENV_DOMAIN
                auth.anonymous:
                  enabled: true
                  hide_version: true
                  org_role: Viewer
              ingress:
                enabled: true
                ingressClassName: traefik
                annotations:
                  traefik.ingress.kubernetes.io/router.entrypoints: websecure
                  traefik.ingress.kubernetes.io/router.middlewares: traefik-forward-auth@kubernetescrd
                  cert-manager.io/cluster-issuer: production
                hosts:
                  - grafana.ARGOCD_ENV_DOMAIN
                path: /
                tls:
                  - secretName: grafana-cert
                    hosts:
                      - grafana.ARGOCD_ENV_DOMAIN
            additionalPrometheusRulesMap:
              rules:
                groups:
                  - name: latency-monitor
                    interval: 30s
                    rules:
                      - alert: LatencyMonitorHighLatency
                        expr: rpch_latencies_success{quantile="0.9"} > 1
                        for: 3m
                        labels:
                          severity: warning
                        annotations:
                          summary: "Latencies are very high."
                          description: "10% of all successful RPCh request latencies (quantile Q90%) have been above 1 second for 3 minutes."
                      - alert: LatencyMonitorHighFailureCount
                        expr: increase(rpch_latencies_failure_count[60s]) > 4
                        for: 3m
                        labels:
                          severity: warning
                        annotations:
                          summary: "Failed latency request count is very high."
                          description: "Failed RPCh latency request count has been more than 4 for 3 minutes."
  destination:
    server: https://kubernetes.default.svc
    namespace: monitoring
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: true
    syncOptions:
      - CreateNamespace=true
      - PruneLast=true
      - ServerSideApply=true